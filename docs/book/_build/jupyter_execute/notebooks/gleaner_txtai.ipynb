{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ta4UtFPOmhEN"
   },
   "source": [
    "# Gleaner & txtai\n",
    "\n",
    "## About\n",
    "\n",
    "Exploring TXTAI (https://github.com/neuml/txtai) as yet another canidate in generating a set of nodes (V) that could be fed into a graph as the initial node set.  Essentially looking at semantic search for the initial full text index search and then moving on to a graph database (triplestore in my case) fort he graph search / ananlysis portion.\n",
    "\n",
    "This is the \"search broker\" concept I've been trying to resolve. \n",
    "\n",
    "## References\n",
    "\n",
    "* https://github.com/neuml/txtai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z52m13kJwMTo"
   },
   "source": [
    "## Imports and Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9Re_OoSDlSVS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\r\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "!pip install -q git+https://github.com/neuml/txtai\n",
    "!pip install -q  'fsspec>=0.3.3'\n",
    "!pip install -q  s3fs\n",
    "!pip install -q  boto3\n",
    "!pip install -q  spacy\n",
    "!pip install -q  pyarrow\n",
    "!pip install -q  fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "QqyuQ2Ofl-fr"
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import pandas as pd\n",
    "import dask, boto3\n",
    "import dask.dataframe as dd\n",
    "from txtai.embeddings import Embeddings\n",
    "\n",
    "# Create embeddings model, backed by sentence-transformers & transformers\n",
    "embeddings = Embeddings({\"method\": \"transformers\", \"path\": \"sentence-transformers/bert-base-nli-mean-tokens\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27XGS8Mk4Bm_"
   },
   "source": [
    "## Gleaner Data\n",
    "\n",
    "First lets load up some of the data Gleaner has collected.  This is just simple data graph objects and not any graphs or other processed products from Gleaner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "_tZgszLR4YSm"
   },
   "outputs": [],
   "source": [
    "# Set up our S3FileSystem object\n",
    "import s3fs \n",
    "oss = s3fs.S3FileSystem(\n",
    "      anon=True,\n",
    "      client_kwargs = {\"endpoint_url\":\"https://oss.geodex.org\"}\n",
    "   )\n",
    "# oss.ls('gleaner/summoned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "0YEib_ce4eu9",
    "outputId": "c1a394c7-7fdf-446f-f711-6a5edf78c0f1"
   },
   "outputs": [],
   "source": [
    "# # A simple example of grabbing one item...  \n",
    "# import json \n",
    "\n",
    "# jld = \"\"\n",
    "# with oss.open('gleaner/summoned/opentopo/231f7fa996be8bd5c28b64ed42907b65cca5ee30.jsonld', 'rb') as f:\n",
    "#   #print(f.read())\n",
    "#    jld = f.read().decode(\"utf-8\", \"ignore\").replace('\\n',' ')\n",
    "#    json = json.loads(jld)\n",
    "\n",
    "# document = json['description']\n",
    "# print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "jta0pzeOKbG0",
    "outputId": "ee31747e-bc21-40ed-c133-37154f0c2e45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indexing gleaner/summoned/opentopo\n",
      "654\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "@dask.delayed()\n",
    "def read_a_file(fn):\n",
    "    # or preferably open in text mode and json.load from the file\n",
    "    with oss.open(fn, 'rb') as f:\n",
    "        #return json.loads(f.read().replace('\\n',' '))\n",
    "        return json.loads(f.read().decode(\"utf-8\", \"ignore\").replace('\\n',' '))\n",
    "\n",
    "# buckets = ['gleaner/summoned/dataucaredu', 'gleaner/summoned/getiedadataorg', 'gleaner/summoned/iris', 'gleaner/summoned/opentopo', 'gleaner/summoned/ssdb', 'gleaner/summoned/wikilinkedearth', 'gleaner/summoned/wwwbco-dmoorg', 'gleaner/summoned/wwwhydroshareorg', 'gleaner/summoned/wwwunavcoorg']\n",
    "\n",
    "buckets = ['gleaner/summoned/opentopo']\n",
    "\n",
    "filenames = []\n",
    "\n",
    "for d in range(len(buckets)):\n",
    "  print(\"indexing {}\".format(buckets[d]))\n",
    "  f = oss.ls(buckets[d])\n",
    "  filenames += f\n",
    "\n",
    "#filenames = oss.cat('gleaner/summoned/opentopo', recursive=True)\n",
    "output = [read_a_file(f) for f in filenames]\n",
    "print(len(filenames))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "Uwjzh7PmK5Z9",
    "outputId": "54c5d5d3-83c8-4e1c-a65f-22cd11363dd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.3 s, sys: 830 ms, total: 13.1 s\n",
      "Wall time: 59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "gldf = pd.DataFrame(columns=['name', 'url', \"keywords\", \"description\", \"object\"])\n",
    "\n",
    "#for key in filenames:\n",
    "\n",
    "for doc in range(len(output)):\n",
    "#for doc in range(10):\n",
    "#for key in filenames:\n",
    "  #if \".jsonld\" in key:\n",
    "  if \"/.jsonld\" not in filenames[doc] :\n",
    "    try:\n",
    "      jld = output[doc].compute()\n",
    "    except:\n",
    "      print(filenames[doc])\n",
    "      print(\"Doc has bad encoding\")\n",
    "\n",
    "    # TODO  Really need to flatten and or frame this\n",
    "    try:\n",
    "      desc = jld[\"description\"]\n",
    "    except:\n",
    "      desc = \"NA\"\n",
    "      continue\n",
    "    kws = \"keywords\" #jld[\"keywords\"]\n",
    "    name = jld[\"name\"]\n",
    "    url = \"NA\" #jld[\"url\"]\n",
    "    object = filenames[doc]\n",
    "\n",
    "    gldf = gldf.append({'name':name, 'url':url, 'keywords':kws, 'description': desc, 'object': object}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "5YBhd00_aLIF",
    "outputId": "a40fedbf-da6b-4e21-8b24-8ba2433d5fbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 654 entries, 0 to 653\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   name         654 non-null    object\n",
      " 1   url          654 non-null    object\n",
      " 2   keywords     654 non-null    object\n",
      " 3   description  654 non-null    object\n",
      " 4   object       654 non-null    object\n",
      "dtypes: object(5)\n",
      "memory usage: 25.7+ KB\n"
     ]
    }
   ],
   "source": [
    "gldf.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "XXTE4TUsT7EL"
   },
   "outputs": [],
   "source": [
    "gldf.to_parquet('index.parquet.gzip',  compression='gzip') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPSce6EfR8bL"
   },
   "source": [
    "## Erratta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "pub17I6ZlaxZ"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text_corpus = []\n",
    "\n",
    "# for i in range(len(gldf)):\n",
    "#   text_corpus += gldf.at[i,'description']\n",
    "\n",
    "# for i in range(len(gldf)):\n",
    "for i in range(10):\n",
    "  d = gldf.at[i,'description']\n",
    "  # d.replace('(', '').replace(')', '').replace('\\\"', '')\n",
    "  dp = re.sub(r'[^A-Za-z0-9 ]+', '', str(d))\n",
    "  text_corpus.append(str(dp))\n",
    "\n",
    "  # if not \"http\" in d:\n",
    "  #   if not \"(\" in d:\n",
    "  #     if not \"<\" in d:\n",
    "  #       text_corpus.append(str(d))\n",
    "\n",
    "# for x in range(len(text_corpus)):\n",
    "#   print(text_corpus[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "T3sDhif_lyw5"
   },
   "outputs": [],
   "source": [
    "# Not needed for textai\n",
    "\n",
    "# Create a set of frequent words\n",
    "stoplist = set('for a of the and to in'.split(' '))\n",
    "# Lowercase each document, split it by white space and filter out stopwords\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "         for document in text_corpus]\n",
    "\n",
    "# Count word frequencies\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "# Only keep words that appear more than once\n",
    "processed_corpus = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "# pprint.pprint(processed_corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bixS1ilUv7ji"
   },
   "source": [
    "## txtai section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "id": "EMd-RIxIzDEC",
    "outputId": "d5d31b35-e694-4935-e9f6-ab75e776da20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score:0.3274398148059845 -- gleaner/summoned/opentopo/04d01beb4b6be2ea15309823124e8029a8547f82.jsonld\n",
      "\n",
      "\n",
      "score:0.263794869184494 -- gleaner/summoned/opentopo/008b91b98f92c4b6110bb40ec1dae10240ec28f0.jsonld\n",
      "\n",
      "\n",
      "score:0.2295398861169815 -- gleaner/summoned/opentopo/04324ac3558c70ed30fbafe4ad62637fd9d2975b.jsonld\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create an index for the list of text_corpus\n",
    "embeddings.index([(gldf.at[uid,'object'], text, None) for uid, text in enumerate(text_corpus)])\n",
    "embeddings.save(\"index\")\n",
    "embeddings = Embeddings()\n",
    "embeddings.load(\"index\")\n",
    "\n",
    "results = embeddings.search(\"lidar data \", 3)\n",
    "for r in results:\n",
    "  uid = r[0]\n",
    "  score = r[1]\n",
    "  print('score:{} -- {}\\n\\n'.format(score, uid)) #text_corpus[uid]))\n",
    "  #print(gldf.at[uid,'object'])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JelST0Dzwynl"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "gleaner_txtai.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}